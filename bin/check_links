#!/usr/bin/env ruby
# check_links -- ODDB -- 27.11.2003 -- hwyss@ywesee.com

$: << File.expand_path('../src', File.dirname(__FILE__))

require 'net/http'
require 'util/oddbconfig'
require 'util/oddbapp'
require 'hpricot'
begin
	require 'testenvironment'
	puts "loaded testenvironment"
rescue LoadError
end

class ImpatientError < StandardError; end
module Net
  class HTTP
    def request(req, body = nil, &block)  # :yield: +response+
      unless started?
        start {
          req['connection'] ||= 'close'
          return request(req, body, &block)
        }
      end
      if proxy_user()
        unless use_ssl?
          req.proxy_basic_auth proxy_user(), proxy_pass()
        end
      end

      req.set_body_internal body
      begin_transport req
        req.exec @socket, @curr_http_version, edit_path(req.path)
        raise ImpatientError if rand(100) > 30
        begin
          res = HTTPResponse.read_new(@socket)
        end while res.kind_of?(HTTPContinue)
        res.reading_body(@socket, req.response_body_permitted?) {
          yield res if block_given?
        }
      end_transport req, res

      res
    end
  end
end

num_threads = 1
flavor = 'gcc'
$respect_session = 100
$server = ODDB::SERVER_NAME
while(arg = ARGV.shift)
	case arg
  when /session\.(\d+)/
    $respect_session = $1.to_i
	when 'session'
		$respect_session = 100
	when /\d+/
		num_threads = [arg.to_i, num_threads].max
  when /server:(.*)/
    $server = $1
	when /[a-z]{3,}/
		flavor = arg
	end
end

module ODDB
	class CrawlerWriter < NullWriter
		def initialize
			@links = []
		end
		def new_linkhandler(handler)
			if(handler && (href = handler.attribute("href")))
				printf("href: %s\n", href.inspect)
				@links.push(href) 
			end
		end
		def send_meta(attrs)
			if(attrs.any? { |key, val| /http-equiv/i.match(key) \
				&& /refresh/i.match(val)} )
				@error_flag = true
			end
		end
		def links
			@links unless @error_flag
		end
	end
	class Crawler
		def initialize(id, flavor)
			@id = id
			@links = [ "/de/#{flavor}" ]
			@visited = []
			@errors = []
			@session = Net::HTTP.new($server)
			@cookies = {}
			@state_id_regexp = Regexp.new('state_id/[0-9\-]*/')
			@load_max = nil
			@load_max_url = nil
			@load_min = nil
			@load_min_url = nil
		end
		def check_links
			@links.each_with_index { |link, idx|
				printf("Session: %3d - requested: %8d: %s\n", @id, idx, link)
				@visited.push(link)
				start = Time.now
				if(html = get_http(link))
					time = Time.now - start
					if(@load_max.nil? || @load_max < time)
						@load_max = time
						@load_max_url = link
					end
					if(@load_min.nil? || @load_min > time)
						@load_min = time
						@load_min_url = link
					end
					printf("Session: %3d - received:  %8d after %1.2fs: %s\n", @id, idx, time, link)
          schedule_links(extract_links(html), idx)
				else
					printf("Session: %3d - ## error:  %8d after %1.2fs: %s\n", @id, idx, Time.now - start, link)
					@errors.push(link)
				end
			}
		end
		def report
			puts "# Session #{@id} ####################################"
			puts "Visited #{@visited.size} links."
			puts "Encountered #{@errors.size} errors:"
			puts @errors
			puts "Shortest Load-Time: #@load_min s (#@load_min_url)"
			puts "Longest  Load-Time: #@load_max s (#@load_max_url)"
		end
		def schedule_links(links, idx)
			links.each { |link|
				if((link.index($server) || link[0] == ?/)\
					&& !link.index('download') \
					&& !@links.include?(link))
					if(link.index('state_id'))
						@links[idx.next, 0] = link
					else
						@links[rand(@links.size), 0] = link
					end
				end
			}
		end
		def extract_links(html)
      hp = Hpricot(html)
      (hp/"//a").collect { |link|
        (href = link.attributes["href"]) && href.gsub(@state_id_regexp, '')
      }.compact
		end
    def respect_session
      rand(100) < $respect_session
    end
		def get_http(link)
			try = 3
			begin
        resp = nil
        begin
          resp = @session.get(link, headers)
        rescue ImpatientError
			    @session = Net::HTTP.new($server)
          retry
        end
				if resp.is_a? Net::HTTPOK
          html = resp.body.dup
					update_cookies(resp) if(respect_session)
					html
				end
			rescue Timeout::Error, EOFError
				if(try > 0)
					sleep 10
					try -= 1
					retry
				end
			rescue StandardError => e
				puts e.message
			end
		end
		def update_cookies(resp)
			if(cookiestring = resp['set-cookie'])
				ptrn = /(?:^|, (?!\d))([^;]+)/
				cookiestring.scan(ptrn) { |cookie|
					@cookies.store(*(cookie[0].split('=',2)))
				}
			end
		end
		def cookies
			@cookies.collect { |var| var.join('=') }.join('; ')
		end
		def headers
			{
				'Cookie'          =>  cookies,
				#'User-Agent'      =>  'Oddb Test-Crawler',
				'User-Agent'			=>	'Mozilla',
			}
		end
	end
end

#trap('INT') { exit }

puts "checking links for #{$server}"
puts "flavor:      #{flavor}"
puts "sessions:    #{$respect_session}%"
puts "concurrent:  #{num_threads}"
crawlers = []
threads = []
num_threads.times { |num|
	threads.push Thread.new {
		crawler = ODDB::Crawler.new(num, flavor)
		crawlers.push(crawler)
		crawler.check_links
	}
}

at_exit {
	crawlers.each { |crawler| crawler.report } 
}

threads.each { |thread|
	thread.join
}
