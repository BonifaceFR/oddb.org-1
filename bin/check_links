#!/usr/bin/env ruby
# check_links -- ODDB -- 27.11.2003 -- hwyss@ywesee.com

$: << File.expand_path('../src', File.dirname(__FILE__))

require 'util/html_parser'
require 'net/http'
require 'util/oddbconfig'
require 'util/oddbapp'
begin
	require 'testenvironment'
rescue LoadError
end

threads = 1
if(ARGV[0] == "session")
	$respect_session = true
	threads = [ARGV[1].to_i, 1].max
else
	$respect_session = false
end

module ODDB
	class CrawlerWriter < NullWriter
		def initialize
			@links = []
		end
		def new_linkhandler(handler)
			@links.push(handler.attribute("href")) if(handler)
		end
		def send_meta(attrs)
			if(attrs.any? { |key, val| /http-equiv/i.match(key) \
				&& /refresh/i.match(val)} )
				@error_flag = true
			end
		end
		def links
			@links unless @error_flag
		end
	end
	class Crawler
		def initialize
			@links = [ '/' ]
			@visited = []
			@errors = []
			puts "checking links for #{SERVER_NAME}"
			@session = Net::HTTP.new(SERVER_NAME)
			@cookies = {}
			@state_id_regexp = Regexp.new('state_id/[0-9]+')
			@load_max = nil
			@load_max_url = nil
			@load_min = nil
			@load_min_url = nil
		end
		def check_links
			@links.each_with_index { |link, idx|
				puts "#{sprintf("%8d", idx)}: #{link}"
				@visited.push(link)
				if(links = check(link))
					schedule_links(links, idx)
				else
					puts "## error: #{link}"
					@errors.push(link)
				end
			}
		ensure
			report
		end
		def report
			puts "Visited #{@visited.size} links."
			puts "Encountered #{@errors.size} errors:"
			puts @errors
			puts "Shortest Load-Time: #@load_min s (#@load_min_url)"
			puts "Longest  Load-Time: #@load_max s (#@load_max_url)"
		end
		def schedule_links(links, idx)
=begin
			puts "scheduling...."
			puts links
			puts "-"*40
			puts @visited
			puts "="*40
			puts links-@visited
			puts "*"*40
=end
			links.each { |link|
				if((link.index(SERVER_NAME) || link[0] == ?/)\
					&& !link.index('download') \
					&& !@links.include?(link))
					if(link.index('state_id'))
						@links[idx.next, 0] = link
					else
						@links[rand(@links.size), 0] = link
					end
				end
			}
		end
		def check(link)
			start = Time.now
			if(html = get_http(link))
				time = Time.now - start
				if(@load_max.nil? || @load_max < time)
					@load_max = time
					@load_max_url = link
				end
				if(@load_min.nil? || @load_min > time)
					@load_min = time
					@load_min_url = link
				end
				puts "received: #{link}"
				writer = CrawlerWriter.new
				formatter = HtmlFormatter.new(writer)
				parser = HtmlParser.new(formatter)
				parser.feed(html)
				links = writer.links.compact
				links.collect { |link| 
					link.gsub(@state_id_regexp, '')
				}
			end
		end
		def get_http(link)
			try = 3
			begin
				continuation = nil
				callcc { |continuation| }	
				resp = @session.get(link, headers)
				if resp.is_a? Net::HTTPOK
					if(rand(100) < 30)
						#impatient 30% that click again
						continuation.call
					end
					update_cookies(resp) if($respect_session)
					resp.body
				end
			rescue Timeout::Error, EOFError
				if(try > 0)
					sleep 10
					try -= 1
					retry
				end
			rescue StandardError => e
				puts e.message
			end
		end
		def update_cookies(resp)
			if(cookiestring = resp['set-cookie'])
				ptrn = /(?:^|, (?!\d))([^;]+)/
				cookiestring.scan(ptrn) { |cookie|
					@cookies.store(*(cookie[0].split('=',2)))
				}
			end
		end
		def cookies
			@cookies.collect { |var| var.join('=') }.join('; ')
		end
		def headers
			{
				'Cookie'          =>  cookies,
				#'User-Agent'      =>  'Oddb Test-Crawler',
				'User-Agent'			=>	'Mozilla',
			}
		end
	end
end

#trap('INT') { exit }

thread = nil
threads.times { |num|
	thread = Thread.new {
		crawler = ODDB::Crawler.new
		crawler.check_links
	}
}

thread.join
